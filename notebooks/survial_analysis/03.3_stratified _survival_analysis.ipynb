{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d1474d-d64a-4efe-b751-f8e084f9a78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6c6189-0643-4bea-9513-a780d8d64a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, hour, month, when, lit, least\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import multivariate_logrank_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928f0902-087a-4381-8005-73b1d3d63d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TORONTO_TABLE = \"workspace.capstone_project.toronto_model_ready\"\n",
    "NYC_TABLE     = \"workspace.capstone_project.nyc_model_ready\"\n",
    "\n",
    "CENSOR_THRESHOLD = 60   # minutes\n",
    "ALPHA = 0.05\n",
    "\n",
    "SAVE_DIR = \"/Workspace/Shared/DAMO_699-4-Capstone-Project/output/graphs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving outputs to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7917f3-a21c-4b4e-9bc5-f94a07d2cdb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Prepare Toronto\n",
    "# -----------------------\n",
    "df_to = spark.read.table(TORONTO_TABLE).select(\n",
    "    col(\"response_minutes\").alias(\"duration_original\"),\n",
    "    col(\"event_indicator\").alias(\"event_original\"),\n",
    "    col(\"hour\"),\n",
    "    col(\"season\"),\n",
    "    col(\"day_of_week\")\n",
    ")\n",
    "\n",
    "df_to = df_to.where(\n",
    "\"duration_original is not null AND duration_original > 0 AND event_original is not null\"\n",
    ")\n",
    "\n",
    "# Apply censoring: duration = min(duration_original, 60)\n",
    "df_to = df_to.withColumn(\n",
    "    \"response_minutes\",\n",
    "    least(col(\"duration_original\"), lit(float(CENSOR_THRESHOLD)))\n",
    ")\n",
    "\n",
    "# Apply censoring event rule:\n",
    "# event = 1 if duration_original <= 60 AND event_original==1 else 0\n",
    "df_to = df_to.withColumn(\n",
    "    \"event_indicator\",\n",
    "    when((col(\"duration_original\") <= CENSOR_THRESHOLD) & (col(\"event_original\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# hour_group bins: Night 00–06, Morning 06–12, Afternoon 12–18, Evening 18–24\n",
    "df_to = df_to.withColumn(\n",
    "    \"hour_group\",\n",
    "    when((col(\"hour\") >= 0) & (col(\"hour\") < 6), \"Night\")\n",
    "    .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"Morning\")\n",
    "    .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"Afternoon\")\n",
    "    .otherwise(\"Evening\")\n",
    ")\n",
    "\n",
    "df_to = df_to.withColumn(\n",
    "    \"day_of_week_name\",\n",
    "    when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "    .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "    .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "    .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "    .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "    .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "    .when(col(\"day_of_week\") == 7, \"Saturday\")\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Prepare NYC\n",
    "# -----------------------\n",
    "df_nyc = spark.read.table(NYC_TABLE).select(\n",
    "    col(\"response_minutes\").alias(\"duration_original\"),\n",
    "    col(\"event_indicator\").alias(\"event_original\"),\n",
    "    col(\"hour\"),\n",
    "    col(\"season\"),\n",
    "    col(\"day_of_week\")\n",
    ")\n",
    "\n",
    "df_nyc = df_nyc.where(\n",
    "\"duration_original is not null AND duration_original > 0 AND event_original is not null\"\n",
    ")\n",
    "\n",
    "# Apply censoring duration\n",
    "df_nyc = df_nyc.withColumn(\n",
    "    \"response_minutes\",\n",
    "    least(col(\"duration_original\"), lit(float(CENSOR_THRESHOLD)))\n",
    ")\n",
    "\n",
    "# Apply censoring event rule\n",
    "df_nyc = df_nyc.withColumn(\n",
    "    \"event_indicator\",\n",
    "    when((col(\"duration_original\") <= CENSOR_THRESHOLD) & (col(\"event_original\") == 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# hour_group bins\n",
    "df_nyc = df_nyc.withColumn(\n",
    "    \"hour_group\",\n",
    "    when((col(\"hour\") >= 0) & (col(\"hour\") < 6), \"Night\")\n",
    "    .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"Morning\")\n",
    "    .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"Afternoon\")\n",
    "    .otherwise(\"Evening\")\n",
    ")\n",
    "df_nyc = df_nyc.withColumn(\n",
    "    \"day_of_week_name\",\n",
    "    when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "    .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "    .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "    .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "    .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "    .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "    .when(col(\"day_of_week\") == 7, \"Saturday\")\n",
    ")\n",
    "\n",
    "print(\"Toronto rows:\", df_to.count())\n",
    "print(\"NYC rows:\", df_nyc.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c3e297-79e0-4e24-96d1-5cec966face7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "def plot_km_and_test(df_spark, city_name, group_col, strat_label, group_order=None):\n",
    "    \n",
    "    df_pd = df_spark.select(\n",
    "        \"response_minutes\",\n",
    "        \"event_indicator\",\n",
    "        group_col\n",
    "    ).toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    if group_order is None:\n",
    "        groups = sorted(df_pd[group_col].dropna().astype(str).unique())\n",
    "    else:\n",
    "        groups = group_order\n",
    "    \n",
    "    for g in groups:\n",
    "        sub = df_pd[df_pd[group_col].astype(str) == str(g)]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        \n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(sub[\"response_minutes\"], sub[\"event_indicator\"], label=str(g))\n",
    "        kmf.plot_survival_function()\n",
    "    \n",
    "    plt.title(f\"{city_name} – KM by {strat_label}\")\n",
    "    plt.xlabel(\"Response Time (minutes)\")\n",
    "    plt.ylabel(\"Survival Probability\")\n",
    "    plt.xlim(0, CENSOR_THRESHOLD)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = f\"{SAVE_DIR}/{city_name.lower()}_km_by_{strat_label.lower().replace(' ','_')}.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Saved:\", save_path)\n",
    "    \n",
    "    # Log-rank test\n",
    "    result = multivariate_logrank_test(\n",
    "        df_pd[\"response_minutes\"],\n",
    "        df_pd[group_col],\n",
    "        df_pd[\"event_indicator\"]\n",
    "    )\n",
    "    \n",
    "    pval = float(result.p_value)\n",
    "    significant = pval < ALPHA\n",
    "    \n",
    "    print(f\"{city_name} – Log-rank p-value ({strat_label}):\", pval)\n",
    "    \n",
    "    return {\n",
    "        \"City\": city_name,\n",
    "        \"Stratification\": strat_label,\n",
    "        \"Test\": \"Log-rank\",\n",
    "        \"p_value\": pval,\n",
    "        \"Significant?\": significant\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "439a430d-9a7c-4c8b-976a-f305026a6d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "hour_order = [\"Night\",\"Morning\",\"Afternoon\",\"Evening\"]\n",
    "season_order = [\"winter\",\"spring\",\"summer\",\"fall\"]\n",
    "dow_order = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "\n",
    "# Toronto\n",
    "results.append(plot_km_and_test(df_to, \"Toronto\", \"hour_group\", \"Hour\", hour_order))\n",
    "results.append(plot_km_and_test(df_to, \"Toronto\", \"season\", \"Season\", season_order))\n",
    "results.append(plot_km_and_test(df_to, \"Toronto\", \"day_of_week_name\", \"Day of Week\", dow_order))\n",
    "\n",
    "# NYC\n",
    "results.append(plot_km_and_test(df_nyc, \"NYC\", \"hour_group\", \"Hour\", hour_order))\n",
    "results.append(plot_km_and_test(df_nyc, \"NYC\", \"season\", \"Season\", season_order))\n",
    "results.append(plot_km_and_test(df_nyc, \"NYC\", \"day_of_week_name\", \"Day of Week\", dow_order))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5c1307-f844-4227-b173-28ddf9ee54c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def logrank_summary(df_spark, city, strat_col):\n",
    "    df_pd = df_spark.select(\"response_minutes\", \"event_indicator\", strat_col).toPandas()\n",
    "\n",
    "    res = multivariate_logrank_test(\n",
    "        df_pd[\"response_minutes\"],\n",
    "        df_pd[strat_col],\n",
    "        df_pd[\"event_indicator\"]\n",
    "    )\n",
    "\n",
    "    pval = float(res.p_value)\n",
    "    significant = pval < ALPHA\n",
    "\n",
    "    # Higher-risk group (tail @60): highest S(60)\n",
    "    risk_group = None\n",
    "    if significant:\n",
    "        s_at_60 = {}\n",
    "        for g in sorted(df_pd[strat_col].dropna().astype(str).unique()):\n",
    "            sub = df_pd[df_pd[strat_col].astype(str) == g]\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "            kmf = KaplanMeierFitter()\n",
    "            kmf.fit(sub[\"response_minutes\"], sub[\"event_indicator\"])\n",
    "            s_at_60[g] = float(kmf.predict(CENSOR_THRESHOLD))\n",
    "\n",
    "        if len(s_at_60) > 0:\n",
    "            risk_group = max(s_at_60, key=s_at_60.get)\n",
    "\n",
    "    # Safe label mapping\n",
    "    if strat_col == \"hour_group\":\n",
    "        label = \"hour\"\n",
    "    elif strat_col == \"season\":\n",
    "        label = \"season\"\n",
    "    elif strat_col in [\"day_of_week\", \"day_of_week_norm\", \"day_of_week_name\"]:\n",
    "        label = \"day of week\"\n",
    "    else:\n",
    "        label = strat_col\n",
    "\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"stratification\": label,\n",
    "        \"test\": \"Log-rank\",\n",
    "        \"p_value\": pval,\n",
    "        \"significant\": significant,\n",
    "        \"higher-risk group (tail @60)\": risk_group\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6abd11de-5adc-4c22-97ce-f339278332d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "results.append(logrank_summary(df_to,  \"Toronto\", \"hour_group\"))\n",
    "results.append(logrank_summary(df_to,  \"Toronto\", \"season\"))\n",
    "results.append(logrank_summary(df_to,  \"Toronto\", \"day_of_week\"))\n",
    "\n",
    "results.append(logrank_summary(df_nyc, \"NYC\",     \"hour_group\"))\n",
    "results.append(logrank_summary(df_nyc, \"NYC\",     \"season\"))\n",
    "results.append(logrank_summary(df_nyc, \"NYC\",     \"day_of_week\"))\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "display(summary_df)\n",
    "summary_path = f\"/Workspace/Shared/DAMO_699-4-Capstone-Project/output/tables/logrank_summary_within_city.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(\"Saved summary:\", summary_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf38e3c6-07f0-4e26-b8cd-c0f7e8acdcda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(summary_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd40414a-8e8f-4730-8c0b-58f5ea217ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"----- US4.2 Interpretation Summary -----\\n\")\n",
    "\n",
    "risk_col = \"Higher-risk group (tail @60)\"\n",
    "has_risk = risk_col in summary_df.columns\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    city = row[\"city\"]\n",
    "    strat = row[\"stratification\"]\n",
    "    pval = row[\"p_value\"]\n",
    "    sig  = row[\"significant\"]\n",
    "    risk = row[\"higher-risk group (tail @60)\"]\n",
    "\n",
    "    risk = row[risk_col] if has_risk else None\n",
    "\n",
    "    if sig:\n",
    "        print(f\"{city} – {strat}:\")\n",
    "        print(\"Log-rank test indicates significant differences (p < 0.05).\")\n",
    "\n",
    "        if has_risk and pd.notna(risk):\n",
    "            print(f\"Higher delay-risk group (longer tail at 60 min): {risk}.\")\n",
    "\n",
    "        print(\"Operational insight: Differences suggest temporal variation in response performance.\")\n",
    "        print(\"Check whether curves diverge early (general speed) or mainly in the tail (extreme delays).\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(f\"{city} – {strat}: No statistically significant differences detected.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03.3_stratified _survival_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
