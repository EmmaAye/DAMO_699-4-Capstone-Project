{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "610d57e0-4266-44a1-ac22-454fc14d450f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Sprint 7 / RQ3 Subtask: Toronto Cross-City Model Transfer\n",
    "# Goal: Train on Toronto -> Test on NYC (generalizability)\n",
    "# Output: Metrics table + saved Delta results\n",
    "# ============================================\n",
    "\n",
    "# 1) Setup and Imports\n",
    "import gc\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, FeatureHasher\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# 2) Config\n",
    "TRAIN_TABLE = \"workspace.capstone_project.toronto_model_ready\"\n",
    "TEST_TABLE  = \"workspace.capstone_project.nyc_model_ready\"\n",
    "\n",
    "TRAIN_CITY = \"Toronto\"\n",
    "TEST_CITY  = \"NYC\"\n",
    "\n",
    "LABEL_COL = \"delay_indicator\"\n",
    "\n",
    "DESIRED_CATEGORICAL = [\"incident_category\", \"season\", \"unified_call_source\", \"location_area\"]\n",
    "DESIRED_NUMERIC     = [\"hour\", \"day_of_week\", \"month\", \"year\", \"unified_alarm_level\",\n",
    "                       \"calls_past_30min\", \"calls_past_60min\"]\n",
    "\n",
    "HASH_DIM = 512\n",
    "SEED = 42\n",
    "TRAIN_FRACTION = 0.5\n",
    "MODEL_TYPE = \"rf\"  # \"rf\" or \"lr\"\n",
    "\n",
    "RF_PARAMS = dict(numTrees=80, maxDepth=6)\n",
    "LR_PARAMS = dict(maxIter=50, regParam=0.05, elasticNetParam=0.0)\n",
    "\n",
    "SAVE_RESULTS_TABLE = \"workspace.capstone_project.transfer_test_toronto_to_nyc_sprint7\"\n",
    "\n",
    "# 3) Helpers\n",
    "def load_and_prepare(table_name: str, city_name: str):\n",
    "    df = spark.table(table_name).filter(col(LABEL_COL).isNotNull())\n",
    "    df = df.withColumn(LABEL_COL, col(LABEL_COL).cast(\"int\"))\n",
    "\n",
    "    existing = set(df.columns)\n",
    "    cat_cols = [c for c in DESIRED_CATEGORICAL if c in existing]\n",
    "    num_cols = [c for c in DESIRED_NUMERIC if c in existing]\n",
    "\n",
    "    if len(cat_cols) + len(num_cols) == 0:\n",
    "        raise ValueError(f\"No expected feature columns found in {table_name}. Columns: {df.columns}\")\n",
    "\n",
    "    dist = df.groupBy(LABEL_COL).count().orderBy(LABEL_COL)\n",
    "    print(f\"\\n{city_name} label distribution:\")\n",
    "    dist.show()\n",
    "\n",
    "    labels = [r[LABEL_COL] for r in dist.select(LABEL_COL).collect()]\n",
    "    if len(labels) < 2:\n",
    "        raise ValueError(f\"{city_name} has only one class in {LABEL_COL}: {labels}\")\n",
    "\n",
    "    keep_cols = list(set(cat_cols + num_cols + [LABEL_COL]))\n",
    "    return df.select(*keep_cols), cat_cols, num_cols\n",
    "\n",
    "def build_pipeline(cat_cols, num_cols):\n",
    "    stages = []\n",
    "\n",
    "    if len(cat_cols) > 0:\n",
    "        hasher = FeatureHasher(inputCols=cat_cols, outputCol=\"categorical_features\", numFeatures=HASH_DIM)\n",
    "        stages.append(hasher)\n",
    "        assembler_inputs = num_cols + [\"categorical_features\"]\n",
    "    else:\n",
    "        assembler_inputs = num_cols\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "    stages.append(assembler)\n",
    "\n",
    "    if MODEL_TYPE == \"rf\":\n",
    "        clf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=LABEL_COL,\n",
    "            probabilityCol=\"probability\",\n",
    "            rawPredictionCol=\"rawPrediction\",\n",
    "            **RF_PARAMS\n",
    "        )\n",
    "    else:\n",
    "        clf = LogisticRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=LABEL_COL,\n",
    "            probabilityCol=\"probability\",\n",
    "            rawPredictionCol=\"rawPrediction\",\n",
    "            **LR_PARAMS\n",
    "        )\n",
    "\n",
    "    stages.append(clf)\n",
    "    return Pipeline(stages=stages)\n",
    "\n",
    "def evaluate(pred_df):\n",
    "    roc_eval = BinaryClassificationEvaluator(labelCol=LABEL_COL, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    pr_eval  = BinaryClassificationEvaluator(labelCol=LABEL_COL, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "    acc_eval = MulticlassClassificationEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    f1_eval  = MulticlassClassificationEvaluator(labelCol=LABEL_COL, predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "    auc_roc = float(roc_eval.evaluate(pred_df))\n",
    "    auc_pr  = float(pr_eval.evaluate(pred_df))\n",
    "    acc     = float(acc_eval.evaluate(pred_df))\n",
    "    f1      = float(f1_eval.evaluate(pred_df))\n",
    "    pos_rate = float(pred_df.agg(F.avg(col(LABEL_COL))).first()[0])\n",
    "\n",
    "    return {\n",
    "        \"train_city\": TRAIN_CITY,\n",
    "        \"test_city\": TEST_CITY,\n",
    "        \"auc_roc\": auc_roc,\n",
    "        \"auc_pr\": auc_pr,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"test_positive_rate\": pos_rate\n",
    "    }\n",
    "\n",
    "# 4) Load and align common columns\n",
    "train_df, train_cat, train_num = load_and_prepare(TRAIN_TABLE, TRAIN_CITY)\n",
    "test_df,  test_cat,  test_num  = load_and_prepare(TEST_TABLE, TEST_CITY)\n",
    "\n",
    "common_cat = sorted(list(set(train_cat).intersection(set(test_cat))))\n",
    "common_num = sorted(list(set(train_num).intersection(set(test_num))))\n",
    "\n",
    "print(\"\\n Common numeric cols     :\", common_num)\n",
    "print(\"Common categorical cols :\", common_cat)\n",
    "\n",
    "if len(common_cat) + len(common_num) == 0:\n",
    "    raise ValueError(\"No common feature columns between train and test tables.\")\n",
    "\n",
    "train_common = train_df.select(*(common_num + common_cat + [LABEL_COL]))\n",
    "test_common  = test_df.select(*(common_num + common_cat + [LABEL_COL]))\n",
    "\n",
    "# 5) Train on Toronto -> Test on NYC\n",
    "train_sample = train_common.sample(withReplacement=False, fraction=TRAIN_FRACTION, seed=SEED)\n",
    "pipeline = build_pipeline(common_cat, common_num)\n",
    "\n",
    "print(f\"\\nTraining on {TRAIN_CITY} (fraction={TRAIN_FRACTION}) ...\")\n",
    "model = pipeline.fit(train_sample)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "print(f\"Testing on {TEST_CITY} ...\")\n",
    "pred = model.transform(test_common).select(LABEL_COL, \"rawPrediction\", \"probability\", \"prediction\")\n",
    "\n",
    "metrics = evaluate(pred)\n",
    "\n",
    "# 6) Results table\n",
    "results_df = spark.createDataFrame([metrics])\n",
    "print(\"\\n Toronto -> NYC Transfer Results:\")\n",
    "display(results_df)\n",
    "\n",
    "# 7) Save results (DoD)\n",
    "(\n",
    "    results_df\n",
    "    .withColumn(\"model_type\", F.lit(MODEL_TYPE))\n",
    "    .withColumn(\"train_fraction\", F.lit(TRAIN_FRACTION))\n",
    "    .withColumn(\"run_ts\", F.current_timestamp())\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(SAVE_RESULTS_TABLE)\n",
    ")\n",
    "\n",
    "print(f\" Results saved to: {SAVE_RESULTS_TABLE}\")\n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "gc.collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "toronto_cross_city_model_transfer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
