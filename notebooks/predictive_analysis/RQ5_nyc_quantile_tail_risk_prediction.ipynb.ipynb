{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70a3d31-07d4-4828-a8a2-65b1795c73a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RQ5 — NYC Tail Risk (Mean vs P90/P95 + Predictive Tail Proxy)\n",
    "# Databricks Notebook (Python) — FULL CODE (copy/paste)\n",
    "#\n",
    "# What this produces:\n",
    "# A) Overall NYC tail metrics: mean, P90, P95, P(response>8)\n",
    "# B) Tail metrics by hour (table for dashboard/report)\n",
    "# C) Predictive tail proxy using GBTRegressor:\n",
    "#    compare actual P90 vs predicted P90 on test set\n",
    "#\n",
    "# Notes:\n",
    "# - This is NOT true quantile regression; it is a tail proxy.\n",
    "# - Works on Serverless (no persist/cache required).\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Load NYC Data\n",
    "# -----------------------------\n",
    "TABLE_NYC = \"workspace.capstone_project.nyc_model_ready\"\n",
    "\n",
    "df = spark.table(TABLE_NYC).filter(F.col(\"response_minutes\").isNotNull())\n",
    "\n",
    "print(\"NYC rows (non-null response_minutes):\", df.count())\n",
    "df.select(\"response_minutes\").summary().show()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Part A — TRUE tail metrics (overall)\n",
    "# -----------------------------\n",
    "df.createOrReplaceTempView(\"nyc\")\n",
    "\n",
    "tail_overall = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  AVG(response_minutes) AS mean_minutes,\n",
    "  percentile_approx(response_minutes, 0.90) AS p90_minutes,\n",
    "  percentile_approx(response_minutes, 0.95) AS p95_minutes,\n",
    "  AVG(CASE WHEN response_minutes > 8 THEN 1 ELSE 0 END) AS prob_over_8\n",
    "FROM nyc\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== NYC Overall Tail Metrics ===\")\n",
    "tail_overall.show(truncate=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Part B — Tail metrics by hour (strong evidence for RQ5)\n",
    "# -----------------------------\n",
    "tail_by_hour = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  hour,\n",
    "  AVG(response_minutes) AS mean_minutes,\n",
    "  percentile_approx(response_minutes, 0.90) AS p90_minutes,\n",
    "  percentile_approx(response_minutes, 0.95) AS p95_minutes,\n",
    "  AVG(CASE WHEN response_minutes > 8 THEN 1 ELSE 0 END) AS prob_over_8\n",
    "FROM nyc\n",
    "GROUP BY hour\n",
    "ORDER BY hour\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== NYC Tail Metrics by Hour ===\")\n",
    "display(tail_by_hour)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Part C — Predictive tail proxy (Teammate Model 2 idea, improved)\n",
    "#    Model: GBTRegressor predicts response_minutes\n",
    "#    Evaluation: compare actual P90 vs predicted P90 on test set\n",
    "# -----------------------------\n",
    "feature_cols = [\"hour\", \"day_of_week\", \"calls_past_30min\"]  # conservative: all exist in NYC\n",
    "\n",
    "# Keep only needed columns + drop nulls\n",
    "df_model = df.select(*(feature_cols + [\"response_minutes\"])).dropna()\n",
    "\n",
    "train_df, test_df = df_model.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=\"response_minutes\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=40,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.05,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "print(\"\\nTraining GBTRegressor tail proxy...\")\n",
    "model_tail = pipeline.fit(train_df)\n",
    "pred = model_tail.transform(test_df)\n",
    "\n",
    "pred.createOrReplaceTempView(\"preds\")\n",
    "\n",
    "tail_compare = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  AVG(response_minutes) AS actual_mean_test,\n",
    "  percentile_approx(response_minutes, 0.90) AS actual_p90_test,\n",
    "  percentile_approx(response_minutes, 0.95) AS actual_p95_test,\n",
    "  percentile_approx(prediction, 0.90) AS predicted_p90_test,\n",
    "  percentile_approx(prediction, 0.95) AS predicted_p95_test\n",
    "FROM preds\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== NYC Predictive Tail Comparison (Test Set) ===\")\n",
    "tail_compare.show(truncate=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Optional: Save model (ONLY if your environment allows UC Volumes)\n",
    "# -----------------------------\n",
    "SAVE_MODEL = False  # set True if you want to save\n",
    "if SAVE_MODEL:\n",
    "    save_path = \"/Volumes/workspace/capstone_project/models/rq5_tail_proxy_gbt_nyc\"\n",
    "    model_tail.write().overwrite().save(save_path)\n",
    "    print(\"Saved NYC RQ5 tail proxy model to:\", save_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Optional: Save a single summary table for report\n",
    "# -----------------------------\n",
    "# Create a compact combined summary for easy export/reporting\n",
    "overall_row = tail_overall.first()\n",
    "compare_row = tail_compare.first()\n",
    "\n",
    "summary = [(\n",
    "    \"NYC\",\n",
    "    float(overall_row[\"mean_minutes\"]),\n",
    "    float(overall_row[\"p90_minutes\"]),\n",
    "    float(overall_row[\"p95_minutes\"]),\n",
    "    float(overall_row[\"prob_over_8\"]),\n",
    "    float(compare_row[\"predicted_p90_test\"]),\n",
    "    float(compare_row[\"predicted_p95_test\"])\n",
    ")]\n",
    "\n",
    "summary_df = spark.createDataFrame(\n",
    "    summary,\n",
    "    [\"city\", \"mean_minutes\", \"p90_minutes\", \"p95_minutes\", \"prob_over_8\", \"predicted_p90_test\", \"predicted_p95_test\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== NYC RQ5 Final Summary Row ===\")\n",
    "display(summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RQ5_nyc_quantile_tail_risk_prediction.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
