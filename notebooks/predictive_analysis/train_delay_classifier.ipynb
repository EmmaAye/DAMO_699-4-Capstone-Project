{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768c6d20-991e-4943-b51b-746ce470c350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1: Setup and Imports\n",
    "import gc\n",
    "from pyspark.ml.feature import VectorAssembler, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 2: Data Loading & Preprocessing\n",
    "print(\"Loading NYC dataset...\")\n",
    "# Using the NYC table\n",
    "df = spark.table(\"workspace.capstone_project.nyc_model_ready\")\n",
    "df = df.filter(col(\"delay_indicator\").isNotNull())\n",
    "\n",
    "# Preprocessing with Feature Hashing to prevent the 1GB overflow error\n",
    "categorical_cols = ['incident_category', 'season', 'unified_call_source', 'location_area']\n",
    "hasher = FeatureHasher(inputCols=categorical_cols, outputCol=\"categorical_features\", numFeatures=512)\n",
    "\n",
    "# Feature Assembly\n",
    "numeric_cols = ['hour', 'day_of_week', 'month', 'year', 'unified_alarm_level', \n",
    "                'calls_past_30min', 'calls_past_60min']\n",
    "assembler = VectorAssembler(inputCols=numeric_cols + [\"categorical_features\"], outputCol=\"features\")\n",
    "\n",
    "# 3: Training with Downsampling\n",
    "# We start with 50% to ensure the session stays alive\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df_sample = train_df.sample(withReplacement=False, fraction=0.5, seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"delay_indicator\", \n",
    "    numTrees=50, \n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[hasher, assembler, rf])\n",
    "\n",
    "print(\"Starting fit on NYC sample...\")\n",
    "model = pipeline.fit(train_df_sample)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 4: Evaluation and Saving\n",
    "predictions = model.transform(test_df)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"delay_indicator\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"NYC Model AUC: {auc}\")\n",
    "\n",
    "save_path = \"/Volumes/workspace/capstone_project/models/delay_classifier_nyc\"\n",
    "print(f\"Saving model to: {save_path}\")\n",
    "model.write().overwrite().save(save_path)\n",
    "\n",
    "# Clean up memory\n",
    "del model\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_delay_classifier",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
