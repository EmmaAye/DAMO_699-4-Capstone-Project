{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce76522f-b886-4575-a273-0bc984047193",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import datetime\n",
    "\n",
    "# 1. Load the NYC Operational Data\n",
    "df = spark.table(\"workspace.capstone_project.nyc_model_ready\")\n",
    "\n",
    "# 2. Feature Selection & Engineering\n",
    "feature_cols = [\n",
    "    \"hour\", \"day_of_week\", \"unified_alarm_level\", \n",
    "    \"calls_past_30min\", \"calls_past_60min\"\n",
    "]\n",
    "label_col = \"delay_indicator\"\n",
    "\n",
    "# Assemble features for MLlib\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_ml = assembler.transform(df)\n",
    "# filter out rows with null labels\n",
    "df_ml = df_ml.filter(df_ml[label_col].isNotNull())\n",
    "\n",
    "# 3. Train the Model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=label_col,\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"risk_probability_vec\",\n",
    "    seed=42\n",
    ")\n",
    "model = rf.fit(df_ml)\n",
    "\n",
    "# 4. Generate \"Next-Day\" Forecast Slots\n",
    "last_ts = df.select(F.max(\"incident_datetime\")).collect()[0][0]\n",
    "forecast_data = []\n",
    "for i in range(1, 25):\n",
    "    next_time = last_ts + datetime.timedelta(hours=i)\n",
    "    forecast_data.append((\n",
    "        next_time, \n",
    "        next_time.hour, \n",
    "        next_time.weekday(), \n",
    "        1,   # Assume base alarm level 1 for forecast\n",
    "        10,  # Estimated baseline call volume\n",
    "        20   # Estimated baseline call volume\n",
    "    ))\n",
    "forecast_base_df = spark.createDataFrame(\n",
    "    forecast_data, \n",
    "    [\"forecast_timestamp\", \"hour\", \"day_of_week\", \"unified_alarm_level\", \"calls_past_30min\", \"calls_past_60min\"]\n",
    ")\n",
    "\n",
    "# Assemble features for forecast data\n",
    "forecast_base_ml = assembler.transform(forecast_base_df)\n",
    "\n",
    "# 5. Run Prediction (Risk Probabilities)\n",
    "predictions = model.transform(forecast_base_ml)\n",
    "\n",
    "# Extract probability of 'Delay' (class 1)\n",
    "extract_prob_udf = F.udf(lambda v: float(v[1]))\n",
    "final_forecast_df = predictions.withColumn(\n",
    "    \"delay_risk_probability\", \n",
    "    extract_prob_udf(\"risk_probability_vec\")\n",
    ").select(\n",
    "    \"forecast_timestamp\", \n",
    "    \"hour\", \n",
    "    \"delay_risk_probability\"\n",
    ")\n",
    "\n",
    "# 6. Store for Dashboard Integration\n",
    "nyc_risk_forecast_output = \"workspace.capstone_project.nyc_risk_forecast_output\"\n",
    "final_forecast_df.write.mode(\"overwrite\").saveAsTable(nyc_risk_forecast_output)\n",
    "\n",
    "print(f\"Operational Risk Forecast complete. Table saved: {nyc_risk_forecast_output}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "risk_forecast_engine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
