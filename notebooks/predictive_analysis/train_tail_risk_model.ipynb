{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c22086-2521-4f45-a32c-4ec0ec5cf82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TailRiskModel\").getOrCreate()\n",
    "\n",
    "# 1. Load Data\n",
    "df = spark.table(\"workspace.capstone_project.nyc_model_ready\")\n",
    "df = df.filter(col(\"response_minutes\").isNotNull())\n",
    "\n",
    "# 2. Features for Tail Risk\n",
    "feature_cols = ['hour', 'day_of_week', 'calls_past_30min', 'unified_alarm_level']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# 3. Baseline regression model for response time.\n",
    "# Tail-risk proxy evaluated via predicted P90 comparison.\n",
    "gbt = GBTRegressor(labelCol=\"response_minutes\", featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "# 4. Training\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "pipeline_tail = Pipeline(stages=[assembler, gbt])\n",
    "model_tail = pipeline_tail.fit(train_df)\n",
    "\n",
    "# 5. Evaluation for RQ5\n",
    "predictions = model_tail.transform(test_df)\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"response_minutes\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "print(\"RMSE:\", evaluator_rmse.evaluate(predictions))\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    labelCol=\"response_minutes\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "print(\"MAE:\", evaluator_mae.evaluate(predictions))\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"response_minutes\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "print(\"R2:\", evaluator_r2.evaluate(predictions))\n",
    "# We look for high predictions (P90) vs the actual mean\n",
    "predictions.createOrReplaceTempView(\"results\")\n",
    "comparison = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        AVG(response_minutes) as actual_mean,\n",
    "        percentile_approx(response_minutes, 0.90) as actual_p90,\n",
    "        percentile_approx(prediction, 0.90) as predicted_p90\n",
    "    FROM results\n",
    "\"\"\")\n",
    "comparison.show()\n",
    "\n",
    "# Save Model\n",
    "model_tail.write().overwrite().save(\"/Volumes/workspace/capstone_project/models/tail_risk_model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_tail_risk_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
