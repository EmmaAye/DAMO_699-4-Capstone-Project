{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef278dc-9b13-4499-8d23-21cdada2defb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "RQ5 Subtask (Baseline): NYC Tail-Risk Model (GBT Regression)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RQ5 Subtask (Baseline): NYC Tail-Risk Model (GBT Regression)\n",
    "# Same approach as teammate:\n",
    "# - Train GBTRegressor on response_minutes\n",
    "# - Compare actual_mean vs predicted_p90 / predicted_p95\n",
    "# ============================================\n",
    "\n",
    "# 1) Setup and Imports\n",
    "import gc\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 2) Config\n",
    "CITY = \"NYC\"\n",
    "TABLE = \"workspace.capstone_project.nyc_model_ready\"\n",
    "TARGET = \"response_minutes\"\n",
    "\n",
    "# Keep features similar to teammate; auto-drop missing columns to avoid unresolved column errors\n",
    "DESIRED_FEATURES = [\"hour\", \"day_of_week\", \"calls_past_30min\", \"unified_alarm_level\"]\n",
    "\n",
    "SEED = 42\n",
    "MAX_ITERS = 30\n",
    "MAX_DEPTH = 6\n",
    "\n",
    "SAVE_MODEL_PATH = \"/Volumes/workspace/capstone_project/models/tail_risk_model_nyc\"\n",
    "SAVE_RESULTS_TABLE = \"workspace.capstone_project.rq5_tailrisk_baseline_nyc\"\n",
    "\n",
    "# 3) Load + Clean\n",
    "df = spark.table(TABLE).filter(col(TARGET).isNotNull())\n",
    "df = df.filter(col(TARGET) > 0)\n",
    "\n",
    "# Ensure feature cols exist\n",
    "existing = set(df.columns)\n",
    "feature_cols = [c for c in DESIRED_FEATURES if c in existing]\n",
    "if len(feature_cols) == 0:\n",
    "    raise ValueError(f\"{CITY}: None of the desired features exist. Available: {df.columns}\")\n",
    "\n",
    "print(f\"{CITY}: Using features = {feature_cols}\")\n",
    "\n",
    "df_model = df.select(*(feature_cols + [TARGET]))\n",
    "\n",
    "# 4) Assemble features + Train\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=TARGET,\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=MAX_ITERS,\n",
    "    maxDepth=MAX_DEPTH,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "train_df, test_df = df_model.randomSplit([0.8, 0.2], seed=SEED)\n",
    "\n",
    "print(f\"Training {CITY} GBT tail-risk baseline...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 5) Predict + Compare mean vs tail of predictions\n",
    "pred = model.transform(test_df).select(TARGET, \"prediction\")\n",
    "\n",
    "summary = (\n",
    "    pred.agg(\n",
    "        F.avg(col(TARGET)).alias(\"actual_mean\"),\n",
    "        F.expr(\"percentile_approx(prediction, 0.90)\").alias(\"predicted_p90\"),\n",
    "        F.expr(\"percentile_approx(prediction, 0.95)\").alias(\"predicted_p95\"),\n",
    "        F.expr(\"percentile_approx(response_minutes, 0.90)\").alias(\"actual_p90\"),\n",
    "        F.expr(\"percentile_approx(response_minutes, 0.95)\").alias(\"actual_p95\"),\n",
    "    )\n",
    "    .withColumn(\"city\", F.lit(CITY))\n",
    "    .withColumn(\"tail_gap_p90_minus_mean\", col(\"predicted_p90\") - col(\"actual_mean\"))\n",
    "    .withColumn(\"tail_gap_p95_minus_mean\", col(\"predicted_p95\") - col(\"actual_mean\"))\n",
    ")\n",
    "\n",
    "print(\" NYC tail-risk baseline summary:\")\n",
    "display(summary)\n",
    "\n",
    "# Optional: RMSE for sanity\n",
    "rmse_eval = RegressionEvaluator(labelCol=TARGET, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rmse_eval.evaluate(pred)\n",
    "print(f\"{CITY} RMSE: {rmse}\")\n",
    "\n",
    "# 6) Save model (Spark-friendly)\n",
    "print(f\"Saving model to: {SAVE_MODEL_PATH}\")\n",
    "model.write().overwrite().save(SAVE_MODEL_PATH)\n",
    "\n",
    "# 7) Save results table (for JIRA documentation)\n",
    "(\n",
    "    summary\n",
    "    .withColumn(\"rmse\", F.lit(float(rmse)))\n",
    "    .withColumn(\"run_ts\", F.current_timestamp())\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(SAVE_RESULTS_TABLE)\n",
    ")\n",
    "\n",
    "print(f\" Results saved to: {SAVE_RESULTS_TABLE}\")\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rq5_nyc_tail_risk_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
